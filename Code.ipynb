{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAC_High\n",
      "BAC_Low\n",
      "BAC_Open\n",
      "BAC_Close\n",
      "BAC_Volume\n",
      "BAC_Adj Close\n",
      "F_High\n",
      "F_Low\n",
      "F_Open\n",
      "F_Close\n",
      "F_Volume\n",
      "F_Adj Close\n",
      "GE_High\n",
      "GE_Low\n",
      "GE_Open\n",
      "GE_Close\n",
      "GE_Volume\n",
      "GE_Adj Close\n",
      "MSFT_High\n",
      "MSFT_Low\n",
      "MSFT_Open\n",
      "MSFT_Close\n",
      "MSFT_Volume\n",
      "MSFT_Adj Close\n",
      "DLTR_High\n",
      "DLTR_Low\n",
      "DLTR_Open\n",
      "DLTR_Close\n",
      "DLTR_Volume\n",
      "DLTR_Adj Close\n",
      "             DLTR_High      future\n",
      "Date                              \n",
      "2015-03-25   82.830002   84.000000\n",
      "2015-03-26   81.470001   83.190002\n",
      "2015-03-27   82.150002   83.250000\n",
      "2015-03-30   82.489998   82.160004\n",
      "2015-03-31   82.440002   81.760002\n",
      "2015-04-01   81.570000   81.480003\n",
      "2015-04-02   81.550003   81.000000\n",
      "2015-04-06   81.779999   79.790001\n",
      "2015-04-07   81.820000   80.970001\n",
      "2015-04-08   83.529999   80.949997\n",
      "2015-04-09   84.000000   81.260002\n",
      "2015-04-10   83.190002   81.379997\n",
      "2015-04-13   83.250000   81.889999\n",
      "2015-04-14   82.160004   80.459999\n",
      "2015-04-15   81.760002   80.150002\n",
      "2015-04-16   81.480003   77.800003\n",
      "2015-04-17   81.000000   77.879997\n",
      "2015-04-20   79.790001   77.809998\n",
      "2015-04-21   80.970001   77.480003\n",
      "2015-04-22   80.949997   77.320000\n",
      "2015-04-23   81.260002   78.349998\n",
      "2015-04-24   81.379997   79.750000\n",
      "2015-04-27   81.889999   79.370003\n",
      "2015-04-28   80.459999   79.680000\n",
      "2015-04-29   80.150002   79.470001\n",
      "2015-04-30   77.800003   78.400002\n",
      "2015-05-01   77.879997   78.400002\n",
      "2015-05-04   77.809998   78.519997\n",
      "2015-05-05   77.480003   78.650002\n",
      "2015-05-06   77.320000   77.699997\n",
      "...                ...         ...\n",
      "2019-02-11   98.230003   97.519997\n",
      "2019-02-12   98.709999   96.599998\n",
      "2019-02-13   98.970001   96.699997\n",
      "2019-02-14   99.099998   97.389999\n",
      "2019-02-15  100.120003   97.050003\n",
      "2019-02-19   99.440002   96.139999\n",
      "2019-02-20  100.010002  101.089996\n",
      "2019-02-21   98.949997  103.510002\n",
      "2019-02-22   98.570000  104.190002\n",
      "2019-02-25   98.400002  103.910004\n",
      "2019-02-26   97.519997  104.489998\n",
      "2019-02-27   96.599998  103.220001\n",
      "2019-02-28   96.699997  101.750000\n",
      "2019-03-01   97.389999  100.790001\n",
      "2019-03-04   97.050003  101.080002\n",
      "2019-03-05   96.139999  102.230003\n",
      "2019-03-06  101.089996  102.320000\n",
      "2019-03-07  103.510002  102.389999\n",
      "2019-03-08  104.190002  102.150002\n",
      "2019-03-11  103.910004  102.559998\n",
      "2019-03-12  104.489998         NaN\n",
      "2019-03-13  103.220001         NaN\n",
      "2019-03-14  101.750000         NaN\n",
      "2019-03-15  100.790001         NaN\n",
      "2019-03-18  101.080002         NaN\n",
      "2019-03-19  102.230003         NaN\n",
      "2019-03-20  102.320000         NaN\n",
      "2019-03-21  102.389999         NaN\n",
      "2019-03-22  102.150002         NaN\n",
      "2019-03-25  102.559998         NaN\n",
      "\n",
      "[1007 rows x 2 columns]\n",
      "BAC_High\n",
      "BAC_Low\n",
      "BAC_Open\n",
      "BAC_Close\n",
      "BAC_Volume\n",
      "BAC_Adj Close\n",
      "F_High\n",
      "F_Low\n",
      "F_Open\n",
      "F_Close\n",
      "F_Volume\n",
      "F_Adj Close\n",
      "GE_High\n",
      "GE_Low\n",
      "GE_Open\n",
      "GE_Close\n",
      "GE_Volume\n",
      "GE_Adj Close\n",
      "MSFT_High\n",
      "MSFT_Low\n",
      "MSFT_Open\n",
      "MSFT_Close\n",
      "MSFT_Volume\n",
      "MSFT_Adj Close\n",
      "DLTR_High\n",
      "DLTR_Low\n",
      "DLTR_Open\n",
      "DLTR_Close\n",
      "DLTR_Volume\n",
      "DLTR_Adj Close\n",
      "future\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laukik/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/laukik/.local/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "440/908 [=============>................] - ETA: 1:08 - loss: 1.4735 - mean_absolute_error: 0.9167"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1282d22b4006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m              metrics=['mae'])\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,BatchNormalization\n",
    "\n",
    "#start = dt.datetime(2016,1,1)\n",
    "#end = dt.datetime(2019,3,31)\n",
    "\n",
    "#ALL CONSTANTS TO BE USED IN PROGRAM\n",
    "SEQ_LEN=90\n",
    "FUTURE_PERIOD_PREDICT=10\n",
    "RATIO_TO_PREDICT=\"DLTR\"\n",
    "\n",
    "#result=web.DataReader('F','yahoo',startDay,endDay)\n",
    "\n",
    "#!pwd\n",
    "\n",
    "#result.to_csv('F.csv') #store in csv file\n",
    "\n",
    "#pd.read_csv('F.csv') #read from csv to pandas dataframe\n",
    "\n",
    "main_df=pd.DataFrame() #initialize empty dataframe\n",
    "\n",
    "ratios=[\"BAC\",\"F\",\"GE\",\"MSFT\",\"DLTR\"]\n",
    "for ratio in ratios:\n",
    "  dataset=f\"{ratio}.csv\"\n",
    "  df=pd.read_csv(dataset)\n",
    "  df.rename(columns={\"High\":f\"{ratio}_High\",\"Low\":f\"{ratio}_Low\",\"Open\":f\"{ratio}_Open\",\n",
    "                    \"Close\":f\"{ratio}_Close\",\"Volume\":f\"{ratio}_Volume\",\"Adj Close\":f\"{ratio}_Adj Close\"},\n",
    "           inplace=True)\n",
    "  df.set_index(\"Date\",inplace=True)\n",
    "  \n",
    "  if len(main_df) ==0:  \n",
    "    main_df=df          #store df in main_df if main_df is empty   \n",
    "  else:\n",
    "    main_df=main_df.join(df) #concate all df dataframes to main_df\n",
    "\n",
    "for c in main_df.columns:\n",
    "  print(c)\n",
    "\n",
    "main_df['future']=main_df[f\"{RATIO_TO_PREDICT}_High\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "#create a future coloumn with value same as specific high coloumn which is shifted up by specific value\n",
    "\n",
    "print(main_df[[f\"{RATIO_TO_PREDICT}_High\",\"future\"]])\n",
    "\n",
    "X=main_df.iloc[:,:-1] #Independent variable\n",
    "\n",
    "for c in X.columns:\n",
    "  print(c)\n",
    "\n",
    "Y=main_df.iloc[:,30:31]  # Dependent coloum in Y\n",
    "\n",
    "for c in Y.columns:\n",
    "  print(c)\n",
    "\n",
    "sc_X=StandardScaler()\n",
    "sc_Y=StandardScaler()\n",
    "\n",
    "X=sc_X.fit_transform(X)\n",
    "\n",
    "Y=sc_Y.fit_transform(Y)\n",
    "\n",
    "X[0][0]\n",
    "\n",
    "Z=np.concatenate((X,Y),axis=1)\n",
    "\n",
    "Z[0]\n",
    "\n",
    "sequential_data=[]\n",
    "prev_days=deque(maxlen=SEQ_LEN)\n",
    "\n",
    "#Creates the complete sequential data to be split into train and test sequences\n",
    "for i in range(0,len(Z)-1):\n",
    "  prev_days.append([n for n in Z[i][:-1]])\n",
    "  if len(prev_days) ==SEQ_LEN:\n",
    "    sequential_data.append([np.array(prev_days),Z[i][-1]])\n",
    "\n",
    "len(sequential_data[916][0])\n",
    "\n",
    "sequential_data_train=[]\n",
    "sequential_data_test=[]\n",
    "sequential_data_train=sequential_data[0:908]\n",
    "sequential_data_test=sequential_data[908:917]\n",
    "\n",
    "type(sequential_data_train[0][0].tolist())\n",
    "\n",
    "x_train=np.empty((908,90,30))\n",
    "y_train=np.empty((908))\n",
    "\n",
    "for i in range(len(sequential_data_train)):\n",
    "  x_train[i]=sequential_data_train[i][0].tolist()\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "for i in range(len(sequential_data_train)):\n",
    "  y_train[i]=sequential_data_train[i][1]\n",
    "\n",
    "y_train[0]\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(90,30),return_sequences=True))  #value should be(90,30)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128,input_shape=(90,30),return_sequences=True))  #value should be(90,30)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128,input_shape=(90,30)))  #value should be(90,30)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "#optimizer\n",
    "#opt=tf.keras.optimizers.Adam(lr=1e-3,decay=1e-5)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "             metrics=['mae'])\n",
    "\n",
    "model.fit(x_train,y_train,batch_size=10,epochs=1)\n",
    "\n",
    "y_pred=sc_Y.inverse_transform(model.predict(x_train))\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(y_pred[0])\n",
    "\n",
    "#create x_test data\n",
    "x_test=np.empty((9,90,30))\n",
    "\n",
    "for i in range(len(sequential_data_test)):\n",
    "    x_test[i]=sequential_data_test[i][0].tolist()\n",
    "\n",
    "print(x_test)\n",
    "\n",
    "y_pred=sc_Y.inverse_transform(model.predict(x_test))\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
